"""
Script ƒë·ªÉ crawl th√¥ng tin cƒÉn h·ªô chung c∆∞ t·∫°i H√† N·ªôi v√† TP.HCM
Crawl apartment information in Hanoi and Ho Chi Minh City

Usage:
    python crawl_chung_cu_hn_hcm.py

T√≠nh nƒÉng:
- Crawl cƒÉn h·ªô chung c∆∞ t·∫°i H√† N·ªôi
- Crawl cƒÉn h·ªô chung c∆∞ t·∫°i TP.HCM
- Xu·∫•t d·ªØ li·ªáu ri√™ng cho m·ªói th√†nh ph·ªë
- Xu·∫•t d·ªØ li·ªáu t·ªïng h·ª£p
- Th·ªëng k√™ chi ti·∫øt
"""

import asyncio
import json
import os
from datetime import datetime
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy


class ChungCuCrawler:
    """Crawler cho cƒÉn h·ªô chung c∆∞ H√† N·ªôi v√† TP.HCM"""

    def __init__(self, headless: bool = True):
        """
        Kh·ªüi t·∫°o crawler

        Args:
            headless: Ch·∫°y browser ·∫©n (m·∫∑c ƒë·ªãnh: True)
        """
        self.browser_config = BrowserConfig(
            headless=headless,
            verbose=True,
            java_script_enabled=True
        )
        self.output_dir = "crawled_data"
        os.makedirs(self.output_dir, exist_ok=True)

    def get_schema(self):
        """
        Schema ƒë·ªÉ tr√≠ch xu·∫•t th√¥ng tin cƒÉn h·ªô

        Returns:
            dict: CSS extraction schema
        """
        return {
            "name": "CƒÉn h·ªô chung c∆∞",
            "baseSelector": "div.re__card-full",
            "fields": [
                {
                    "name": "title",
                    "selector": "a.pr-title",
                    "type": "text",
                },
                {
                    "name": "link",
                    "selector": "a.pr-title",
                    "type": "attribute",
                    "attribute": "href"
                },
                {
                    "name": "price",
                    "selector": "span.re__card-config-price",
                    "type": "text",
                },
                {
                    "name": "area",
                    "selector": "span.re__card-config-area",
                    "type": "text",
                },
                {
                    "name": "location",
                    "selector": "div.re__card-location",
                    "type": "text",
                },
                {
                    "name": "bedrooms",
                    "selector": "span.re__card-config-bedroom",
                    "type": "text",
                },
                {
                    "name": "toilets",
                    "selector": "span.re__card-config-toilet",
                    "type": "text",
                },
                {
                    "name": "description",
                    "selector": "div.re__card-description",
                    "type": "text",
                },
                {
                    "name": "image",
                    "selector": "img.re__card-image",
                    "type": "attribute",
                    "attribute": "src"
                },
                {
                    "name": "publish_date",
                    "selector": "span.re__card-published-info-date",
                    "type": "text",
                }
            ]
        }

    async def crawl_pages(self, base_url: str, num_pages: int, city_name: str):
        """
        Crawl nhi·ªÅu trang

        Args:
            base_url: URL c∆° s·ªü
            num_pages: S·ªë trang c·∫ßn crawl
            city_name: T√™n th√†nh ph·ªë (ƒë·ªÉ hi·ªÉn th·ªã)

        Returns:
            List c√°c cƒÉn h·ªô
        """
        print(f"\n{'='*70}")
        print(f"ƒêang crawl cƒÉn h·ªô chung c∆∞ t·∫°i {city_name}")
        print(f"URL: {base_url}")
        print(f"S·ªë trang: {num_pages}")
        print(f"{'='*70}\n")

        all_properties = []
        extraction_strategy = JsonCssExtractionStrategy(self.get_schema())

        async with AsyncWebCrawler(config=self.browser_config) as crawler:
            for page_num in range(1, num_pages + 1):
                # X√¢y d·ª±ng URL cho m·ªói trang
                if page_num == 1:
                    url = base_url
                else:
                    url = f"{base_url}/p{page_num}"

                print(f"üìÑ Trang {page_num}/{num_pages}: {url}")

                crawler_config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    extraction_strategy=extraction_strategy,
                    wait_until="networkidle",
                    delay_before_return_html=2
                )

                try:
                    result = await crawler.arun(url=url, config=crawler_config)

                    if result.success:
                        try:
                            properties = json.loads(result.extracted_content)
                            all_properties.extend(properties)
                            print(f"   ‚úÖ T√¨m th·∫•y {len(properties)} cƒÉn h·ªô")
                        except json.JSONDecodeError as e:
                            print(f"   ‚ùå L·ªói parse JSON: {e}")
                    else:
                        print(f"   ‚ùå Kh√¥ng crawl ƒë∆∞·ª£c trang n√†y")
                        if page_num > 1:
                            # C√≥ th·ªÉ ƒë√£ h·∫øt trang
                            break

                except Exception as e:
                    print(f"   ‚ùå L·ªói: {e}")

                # Delay gi·ªØa c√°c trang
                if page_num < num_pages:
                    await asyncio.sleep(2)

        print(f"\n‚úÖ Ho√†n th√†nh! T·ªïng c·ªông: {len(all_properties)} cƒÉn h·ªô t·ª´ {city_name}\n")
        return all_properties

    def save_json(self, data, filename):
        """
        L∆∞u d·ªØ li·ªáu ra file JSON

        Args:
            data: D·ªØ li·ªáu c·∫ßn l∆∞u
            filename: T√™n file
        """
        filepath = os.path.join(self.output_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return filepath

    def print_statistics(self, properties, city_name):
        """
        In th·ªëng k√™

        Args:
            properties: Danh s√°ch cƒÉn h·ªô
            city_name: T√™n th√†nh ph·ªë
        """
        print(f"\n{'='*70}")
        print(f"üìä TH·ªêNG K√ä - {city_name.upper()}")
        print(f"{'='*70}")
        print(f"T·ªïng s·ªë cƒÉn h·ªô: {len(properties)}")

        # Th·ªëng k√™ theo ƒë·ªãa ƒëi·ªÉm
        locations = {}
        for prop in properties:
            loc = prop.get('location', 'Kh√¥ng r√µ')
            if loc:
                loc = loc.strip()
                locations[loc] = locations.get(loc, 0) + 1

        if locations:
            print(f"\nüìç Top 10 ƒë·ªãa ƒëi·ªÉm nhi·ªÅu cƒÉn h·ªô nh·∫•t:")
            sorted_locs = sorted(locations.items(), key=lambda x: x[1], reverse=True)[:10]
            for i, (loc, count) in enumerate(sorted_locs, 1):
                print(f"   {i}. {loc}: {count} cƒÉn h·ªô")

        # Th·ªëng k√™ gi√°
        prices_with_values = []
        for prop in properties:
            price_str = prop.get('price', '')
            if price_str and price_str.strip():
                prices_with_values.append(price_str)

        print(f"\nüí∞ CƒÉn h·ªô c√≥ th√¥ng tin gi√°: {len(prices_with_values)}/{len(properties)}")

        # Th·ªëng k√™ di·ªán t√≠ch
        areas = [p.get('area', '') for p in properties if p.get('area')]
        print(f"üìê CƒÉn h·ªô c√≥ th√¥ng tin di·ªán t√≠ch: {len(areas)}/{len(properties)}")

        # Th·ªëng k√™ ph√≤ng ng·ªß
        bedrooms_count = {}
        for prop in properties:
            br = prop.get('bedrooms', '').strip()
            if br:
                bedrooms_count[br] = bedrooms_count.get(br, 0) + 1

        if bedrooms_count:
            print(f"\nüõèÔ∏è  Ph√¢n b·ªï theo s·ªë ph√≤ng ng·ªß:")
            sorted_br = sorted(bedrooms_count.items(), key=lambda x: x[1], reverse=True)
            for br, count in sorted_br:
                print(f"   {br}: {count} cƒÉn h·ªô")


async def main():
    """
    H√†m ch√≠nh - Crawl cƒÉn h·ªô chung c∆∞ H√† N·ªôi v√† TP.HCM
    """
    print("\n" + "="*70)
    print("üè¢ CRAWLER TH√îNG TIN CƒÇN H·ªò CHUNG C∆Ø H√Ä N·ªòI & TP.HCM")
    print("="*70)

    crawler = ChungCuCrawler(headless=True)

    # ========================================
    # C·∫§U H√åNH
    # ========================================

    # S·ªë trang c·∫ßn crawl cho m·ªói th√†nh ph·ªë
    NUM_PAGES = 5  # Thay ƒë·ªïi s·ªë n√†y ƒë·ªÉ crawl nhi·ªÅu ho·∫∑c √≠t trang h∆°n

    # URLs
    HANOI_URL = "https://batdongsan.com.vn/ban-can-ho-chung-cu-ha-noi"
    HCM_URL = "https://batdongsan.com.vn/ban-can-ho-chung-cu-tp-hcm"

    # ========================================
    # CRAWL H√Ä N·ªòI
    # ========================================

    hanoi_properties = await crawler.crawl_pages(
        base_url=HANOI_URL,
        num_pages=NUM_PAGES,
        city_name="H√† N·ªôi"
    )

    # L∆∞u d·ªØ li·ªáu H√† N·ªôi
    if hanoi_properties:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        hanoi_file = crawler.save_json(
            hanoi_properties,
            f"chung_cu_ha_noi_{timestamp}.json"
        )
        print(f"üíæ ƒê√£ l∆∞u d·ªØ li·ªáu H√† N·ªôi: {hanoi_file}")
        crawler.print_statistics(hanoi_properties, "H√† N·ªôi")
    else:
        print("‚ö†Ô∏è  Kh√¥ng c√≥ d·ªØ li·ªáu H√† N·ªôi")

    # ========================================
    # CRAWL TP.HCM
    # ========================================

    hcm_properties = await crawler.crawl_pages(
        base_url=HCM_URL,
        num_pages=NUM_PAGES,
        city_name="TP. H·ªì Ch√≠ Minh"
    )

    # L∆∞u d·ªØ li·ªáu TP.HCM
    if hcm_properties:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        hcm_file = crawler.save_json(
            hcm_properties,
            f"chung_cu_hcm_{timestamp}.json"
        )
        print(f"üíæ ƒê√£ l∆∞u d·ªØ li·ªáu TP.HCM: {hcm_file}")
        crawler.print_statistics(hcm_properties, "TP. H·ªì Ch√≠ Minh")
    else:
        print("‚ö†Ô∏è  Kh√¥ng c√≥ d·ªØ li·ªáu TP.HCM")

    # ========================================
    # T·ªîNG H·ª¢P
    # ========================================

    if hanoi_properties or hcm_properties:
        # T·ªïng h·ª£p t·∫•t c·∫£
        all_properties = {
            "timestamp": datetime.now().isoformat(),
            "total_count": len(hanoi_properties) + len(hcm_properties),
            "hanoi": {
                "count": len(hanoi_properties),
                "properties": hanoi_properties
            },
            "hcm": {
                "count": len(hcm_properties),
                "properties": hcm_properties
            }
        }

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        combined_file = crawler.save_json(
            all_properties,
            f"chung_cu_tong_hop_{timestamp}.json"
        )

        print(f"\n{'='*70}")
        print(f"üìä T·ªîNG K·∫æT")
        print(f"{'='*70}")
        print(f"H√† N·ªôi: {len(hanoi_properties)} cƒÉn h·ªô")
        print(f"TP.HCM: {len(hcm_properties)} cƒÉn h·ªô")
        print(f"T·ªïng c·ªông: {len(hanoi_properties) + len(hcm_properties)} cƒÉn h·ªô")
        print(f"\nüíæ File t·ªïng h·ª£p: {combined_file}")
        print(f"{'='*70}\n")

        # Hi·ªÉn th·ªã m·∫´u
        if hanoi_properties:
            print("üìã M·∫´u cƒÉn h·ªô H√† N·ªôi:")
            print(json.dumps(hanoi_properties[0], ensure_ascii=False, indent=2))
            print()

        if hcm_properties:
            print("üìã M·∫´u cƒÉn h·ªô TP.HCM:")
            print(json.dumps(hcm_properties[0], ensure_ascii=False, indent=2))
            print()

    print("‚úÖ Ho√†n th√†nh!")


if __name__ == "__main__":
    asyncio.run(main())
